{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a748b9a3-363d-41dd-bad0-b362bb5e6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "import csv\n",
    "import pdb\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from PIL import Image\n",
    "\n",
    "from models.spatial_transforms import *\n",
    "from models.temporal_transforms import *\n",
    "from models import models as TSN_model\n",
    "\n",
    "from models.action_vst import action_vst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e26bd9-24d7-419c-a623-dfd9aadb2cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "\n",
    "best_acc = 0.\n",
    "# seed = 1\n",
    "# torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "clip_gradient = 8\n",
    "lr = 0.01\n",
    "lr_steps = [5,10,15]\n",
    "step_size = 10\n",
    "weight_decay = 5e-4\n",
    "batch_size = 4\n",
    "\n",
    "input_mean=[.485, .456, .406]\n",
    "input_std=[.229, .224, .225]\n",
    "normalize = GroupNormalize(input_mean, input_std)\n",
    "\n",
    "scales = [1, .875, .75, .66]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "display=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54e069-df60-46f5-b978-df8e3233c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset_video(Dataset):\n",
    "    def __init__(self, frame_path, spatial_transform, temporal_transform):\n",
    "        self.frame_path = frame_path\n",
    "        self.rgb_samples, self.labels = [], []\n",
    "        for i in os.listdir(os.path.join(frame_path, 'NonViolence_Keyframes')):\n",
    "            self.rgb_samples.append('NonViolence_Keyframes/'+ i)\n",
    "            self.labels.append(0)\n",
    "        for j in os.listdir(os.path.join(frame_path, 'Violence_Keyframes')):\n",
    "            self.rgb_samples.append('Violence_Keyframes/'+ j)\n",
    "            self.labels.append(1)\n",
    "        self.sample_num = len(self.rgb_samples)\n",
    "        self.spatial_transform = spatial_transform\n",
    "        self.temporal_transform = temporal_transform\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_name = self.rgb_samples[idx]\n",
    "        label = self.labels[idx]\n",
    "        clip_rgb_frames = []\n",
    "        clip_depth_frames = []\n",
    "        \n",
    "        rgb_video = rgb_name[rgb_name.find('/')+1:rgb_name.rfind('-')+1] # +1 to advoid error prefix\n",
    "        rgb_folder = os.path.join(self.frame_path, rgb_name[:rgb_name.rfind('/')])\n",
    "        for i in os.listdir(rgb_folder):\n",
    "            if i.startswith(rgb_video):\n",
    "                rgb_cache = Image.open(os.path.join(rgb_folder, i)).convert(\"RGB\")\n",
    "                clip_rgb_frames.append(rgb_cache)\n",
    "        clip_rgb_frames = self.spatial_transform(clip_rgb_frames)\n",
    "        n, h, w = clip_rgb_frames.size()\n",
    "        return clip_rgb_frames.view(3, -1, h, w), int(label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(self.sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a850da11-021b-477e-a021-070e3646f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_train  = torchvision.transforms.Compose([\n",
    "                        GroupMultiScaleCrop(224, scales),\n",
    "                        Stack(roll=True),\n",
    "                        ToTorchFormatTensor(div=True),\n",
    "                        normalize\n",
    "                        ])\n",
    "temporal_transform_train = torchvision.transforms.Compose([\n",
    "                                    TemporalUniformCrop_train(8)\n",
    "                                    ])   \n",
    "\n",
    "dataset = dataset_video('data/RLVS', spatial_transform=trans_train, temporal_transform = temporal_transform_train)\n",
    "# dataset = dataset_video('data/RWF', spatial_transform=trans_train, temporal_transform = temporal_transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3c5bf-971b-45ac-97bf-ccedc76c64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24313f37-f75e-41c8-801a-18510b1a71f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = action_vst(num_classes=2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f19c4e-e612-4949-81d6-4e41abe2b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = lr, momentum = 0.9, weight_decay=weight_decay)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold=0.01, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b77939-08db-479c-8acc-bbad851f7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# log_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "# log_filepath = \"logs/\"+log_time+\".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e5ee5-9385-4c80-a71a-407cf12b1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def train_model(train_loader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i, data in enumerate(tqdm(train_loader, desc='Training'), 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "    train_prec, train_rec, train_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    log = 'Training Loss: {:.4f} Acc: {:.4f} Prec: {:.4f} Rec: {:.4f} F1: {:.4f}'.format(train_loss, train_acc, train_prec, train_rec, train_f1)\n",
    "    print(log)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def evaluate_model(val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            y_true += labels.tolist()\n",
    "            y_pred += predicted.tolist()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    log = 'Validation Loss: {:.4f} Acc: {:.4f} Prec: {:.4f} Rec: {:.4f} F1: {:.4f}'.format(avg_loss, accuracy, precision, recall, f1)\n",
    "    print(log)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff85a33-ebdd-4a00-953d-fcc2e8b94e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 120\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_model(train_loader)\n",
    "    val_acc = evaluate_model(test_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    log = 'Epoch [{}/{}], Train Loss: {:.4f}, Val Acc: {:.4f}'.format(\n",
    "        epoch+1, num_epochs, train_loss, val_acc)\n",
    "    print(log)\n",
    "    # with open(log_filepath, \"a\") as f:\n",
    "    #     f.write(log)\n",
    "    #     f.write('\\n')\n",
    "    scheduler.step(val_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc7a55-5fc4-41c9-beb1-636b6bedb4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
